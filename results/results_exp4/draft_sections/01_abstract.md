## Abstract (Draft)
We evaluate failure recovery overhead in distributed data-parallel training under periodic checkpointing, comparing blocking and overlapped checkpoint strategies across 8 suites (CIFAR-10/CIFAR-100, ResNet-18/ResNet-50, two failure schedules). All runs preserve correctness (pass rate 1.0) and exhibit the expected two restart events under injected failures. Relative to reference no-failure runs, blocking and overlapped strategies reduce goodput by 13.25% and 11.50% on average, respectively. Overlapped checkpointing improves goodput over blocking by 1.99% on average, with gains ranging from 0.24% to 3.65% depending on model/dataset/failure schedule. These gains align with reduced checkpoint-induced stall time (average 10.22s lower than blocking), while divergence metrics remain close between methods.
